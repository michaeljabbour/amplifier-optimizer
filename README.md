# ðŸš€ Amplifier Observability

> **Mission control for your AI agent sessions** - Real-time cost, speed, and trajectory tracking

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Amplifier](https://img.shields.io/badge/amplifier-compatible-green.svg)](https://github.com/microsoft/amplifier)

---

## ðŸ“– Overview

Ever wonder what your AI agent is doing, where it's going, or how much it's costing? **Amplifier Observability** gives you complete visibility into your agent sessions with three core capabilities:

| Capability | What You Get |
|------------|--------------|
| ðŸ’° **Cost/Speed Transparency** | Real-time API costs, token usage, and operation timing |
| ðŸŽ¯ **Time/Task Awareness** | Automatic workflow phase detection (exploration â†’ implementation â†’ verification) |
| ðŸ›°ï¸ **Live Trajectory Tracking** | "Space shuttle mission control" view of where your agent is heading and why |

Built following Amplifier's philosophy: **mechanisms, not policies**. This bundle gives you *awareness*, not *control*.

---

## âœ¨ Key Features

- âš¡ **Zero configuration** - Works out of the box with sensible defaults
- ðŸŽ¯ **Ephemeral awareness** - Agents see metrics without polluting conversation history
- ðŸš€ **Phase detection** - Automatically identifies workflow phase from tool patterns
- ðŸ’° **Cost tracking** - Per-model pricing with configurable threshold warnings
- â±ï¸ **Performance monitoring** - Tool timing with slow operation alerts
- ðŸ“Š **Session summaries** - Comprehensive reports when sessions complete
- ðŸ”§ **Modular design** - Use cost tracking, trajectory, or both independently

---

## ðŸŽ¬ Quick Start

### Installation

```bash
# Add the bundle from GitHub
amplifier bundle add git+https://github.com/michaeljabbour/amplifier-optimizer@main

# Activate it
amplifier bundle use observability

# Start a session - tracking is now active!
amplifier
```

That's it! All your sessions now have mission control visibility.

### Testing First?

```bash
# Run a quick test to verify the bundle works
amplifier bundle add git+https://github.com/michaeljabbour/amplifier-optimizer@main
cd ~/.amplifier/cache/amplifier-optimizer-*/
python modules/observability/tests/test_basic_import.py
```

### Local Development

```bash
# Clone for local development
git clone https://github.com/michaeljabbour/amplifier-optimizer.git
cd amplifier-optimizer

# Test the modules
python modules/observability/tests/test_basic_import.py

# Use locally (bypasses bundle registry)
amplifier --bundle modules/observability
```

---

## ðŸ“Š What You'll See

### During Your Session

#### Phase Transitions
```
ðŸš€ Phase: Exploration â†’ Discovering codebase structure
ðŸš€ Phase: Analysis â†’ Understanding code and requirements
ðŸš€ Phase: Implementation â†’ Writing code
ðŸš€ Phase: Verification â†’ Testing and validation
```

#### Real-Time Warnings
```
ðŸ’° Cost threshold exceeded: $1.0234
âš ï¸ Slow tool: grep took 12.3s
```

#### Trajectory Updates (Every 3 turns)
```
ðŸš€ Trajectory Awareness (Ephemeral)
â”œâ”€ Phase: Implementation (Writing code)
â”œâ”€ Confidence: 85%
â”œâ”€ Duration: 45s in this phase
â”œâ”€ Recent Tools: write_file, edit_file, bash
â””â”€ Predicted Path: **implementation** â†’ verification â†’ debugging

This is mission control - you're on track. Continue your current work.
```

#### Metrics Updates (Every 5 turns)
```
ðŸ“Š Session Metrics (Ephemeral)
â”œâ”€ Cost: $0.0234
â”œâ”€ Tokens: 12,345 (8,234 in, 4,111 out)
â”œâ”€ Time: 78s elapsed
â”œâ”€ Tools: 3 types used, avg 1.2s per call
â””â”€ Turn: 5

This is live awareness - metrics update automatically. Focus on your task.
```

### At Session End

```
================================================================================
ðŸ“Š SESSION SUMMARY
================================================================================

ðŸ’° Cost: $0.0234
ðŸŽ¯ Tokens: 12,345
   â”œâ”€ Input:  8,234
   â””â”€ Output: 4,111

â±ï¸  Time: 124s (2.1 min)

ðŸ”§ Tool Usage:
   read_file            12Ã— calls, avg: 0.45s, total: 5.4s
   write_file            3Ã— calls, avg: 0.23s, total: 0.7s
   bash                  2Ã— calls, avg: 1.82s, total: 3.6s
```

---

## ðŸŽ¯ Real-World Example

**Task:** *"Refactor the authentication module to use OAuth2"*

```
Turn 1: User gives task
â””â”€ Agent starts working

Turn 3: First trajectory update
ðŸš€ Trajectory: **exploration** â†’ analysis â†’ planning
â”œâ”€ Phase: Exploration (Discovering codebase structure)
â”œâ”€ Recent Tools: glob, read_file, grep
â””â”€ Confidence: 78%

Turn 5: First metrics update
ðŸ“Š Metrics
â”œâ”€ Cost: $0.01
â”œâ”€ Time: 23s elapsed
â””â”€ Tokens: 3,456

Turn 8: Phase transition
ðŸš€ Phase: Analysis â†’ Understanding code and requirements

Turn 11: Another phase transition
ðŸš€ Phase: Planning â†’ Designing solution architecture

Turn 14: 
ðŸš€ Phase: Implementation â†’ Writing code
ðŸ’° Cost threshold exceeded: $0.10

Turn 18:
ðŸš€ Phase: Verification â†’ Testing and validation
âš ï¸ Slow tool: bash took 11.2s  â† Running tests

Turn 20: Session complete
ðŸ“Š SESSION SUMMARY
ðŸ’° Cost: $0.12
â±ï¸  Time: 8.5 min
âœ… OAuth2 authentication implemented and tested
```

**What your son learns:**
- The task cost $0.12 (not $0.01 or $10)
- It took 8.5 minutes start to finish
- The agent followed a logical path: explore â†’ analyze â†’ plan â†’ implement â†’ verify
- Tests were slow (11.2s) - maybe optimize next time

---

## ðŸ—ï¸ How It Works

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User: "Refactor the auth module"                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Amplifier Agent Session                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Agent thinks and acts                                â”‚   â”‚
â”‚  â”‚  â”œâ”€ Uses tools (read_file, write_file, bash)         â”‚   â”‚
â”‚  â”‚  â”œâ”€ Calls LLM APIs                                    â”‚   â”‚
â”‚  â”‚  â””â”€ Gets periodic awareness updates (ephemeral)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Observability Hooks (invisible background)          â”‚   â”‚
â”‚  â”‚                                                        â”‚   â”‚
â”‚  â”‚  ObservabilityHook:                                   â”‚   â”‚
â”‚  â”‚  â”œâ”€ Listens to tool:pre/post â†’ tracks timing         â”‚   â”‚
â”‚  â”‚  â”œâ”€ Listens to provider:post â†’ tracks cost/tokens    â”‚   â”‚
â”‚  â”‚  â””â”€ Injects metrics every 5 turns (ephemeral)        â”‚   â”‚
â”‚  â”‚                                                        â”‚   â”‚
â”‚  â”‚  TrajectoryAnalyzer:                                  â”‚   â”‚
â”‚  â”‚  â”œâ”€ Listens to tool:post â†’ builds tool history       â”‚   â”‚
â”‚  â”‚  â”œâ”€ Detects phase from patterns                      â”‚   â”‚
â”‚  â”‚  â””â”€ Injects trajectory every 3 turns (ephemeral)     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User sees:                                                 â”‚
â”‚  - Phase transitions (ðŸš€)                                   â”‚
â”‚  - Cost warnings (ðŸ’°)                                        â”‚
â”‚  - Slow tool warnings (âš ï¸)                                  â”‚
â”‚  - Final summary (ðŸ“Š)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Workflow Phases

The system automatically detects these phases from tool usage:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Exploration â”‚ â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                     â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Analysis â”‚ â”€â”€â”€â”€â”€â”
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚ Planning â”‚ â”€â”€â”€â”€â”€â”
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                           â–¼
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚ Implementation â”‚ â”€â”€â”€â”€â”€â”
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
                                                          â–¼
                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                 â”‚ Verification â”‚
                                                 â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                                  â–¼                           â–¼
                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 âœ… Success
                            â”‚ Debuggingâ”‚
                            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                  â”‚
                                  â””â”€â”€â”€â”€â”€â†’ (back to Implementation)
```

---

## âš™ï¸ Configuration

### Default Settings

Works great out of the box, but you can customize:

```yaml
modules:
  hooks:
    - name: observability
      config:
        model: "claude-sonnet-4-5"  # Model for cost calculations
        cost_threshold: 1.0          # Warn at $1, then $2, $3...
        speed_threshold: 10.0        # Warn if tool takes >10s
        inject_frequency: 5          # Inject metrics every 5 turns
        
    - name: trajectory
      config:
        window_size: 10              # Analyze last 10 tools
        inject_frequency: 3          # Inject trajectory every 3 turns
```

### Budget-Focused Configuration

```yaml
- name: observability
  config:
    cost_threshold: 0.25  # Warn every $0.25
    inject_frequency: 3   # More frequent cost awareness
```

### Performance-Focused Configuration

```yaml
- name: observability
  config:
    speed_threshold: 5.0  # Strict timing requirements
```

### Supported Models

Built-in pricing for:
- Claude Sonnet 4/4.5, Opus 4, Haiku 3
- GPT-4, GPT-4 Turbo, GPT-4o, GPT-4o Mini
- GPT-3.5 Turbo

*See `modules/observability_hook.py` for full pricing table*

---

## ðŸ§ª Testing

Verify installation:

```bash
python modules/observability/tests/test_basic_import.py
```

Expected output:
```
âœ… ObservabilityHook instantiation test passed
âœ… TrajectoryAnalyzer instantiation test passed
âœ… Pricing table test passed
âœ… Phase definitions test passed

âœ… ALL TESTS PASSED
```

---

## ðŸ“‚ Project Structure

```
amplifier-optimizer/
â””â”€â”€ modules/
    â””â”€â”€ observability/
        â”œâ”€â”€ bundle.yaml                           # Bundle composition
        â”œâ”€â”€ modules/
        â”‚   â”œâ”€â”€ __init__.py                       # Module exports
        â”‚   â”œâ”€â”€ observability_hook.py             # Cost/speed/token tracking
        â”‚   â””â”€â”€ trajectory_analyzer.py            # Phase detection
        â”œâ”€â”€ context/
        â”‚   â””â”€â”€ observability-instructions.md     # Agent guidance
        â”œâ”€â”€ tests/
        â”‚   â””â”€â”€ test_basic_import.py              # Basic tests
        â”œâ”€â”€ README.md                             # Full documentation
        â””â”€â”€ QUICKSTART.md                         # 5-minute setup guide
```

**Total code**: ~500 lines across 2 modules  
**Dependencies**: Python stdlib + Amplifier Core only  
**Complexity**: Minimal - ruthlessly simple design

---

## ðŸŽ¨ Design Philosophy

Built following **The Amplifier Way**:

| Principle | Implementation |
|-----------|----------------|
| **Mechanism, not policy** | Provides observability primitives, doesn't enforce budgets |
| **Event-driven** | Built entirely on Amplifier's hooks system |
| **Composable** | Each hook works independently |
| **Optional** | Add/remove without breaking anything |
| **Simple** | ~500 lines, no external dependencies |
| **Context sinks** | Ephemeral awareness doesn't pollute history |

### Why Hooks?

Hooks are **passive observers** - they watch without interfering. The agent gets automatic awareness without needing to remember to check metrics.

### Why Ephemeral Injection?

**Zero clutter** - metrics don't pollute conversation history. The agent has temporary awareness that updates automatically, like a HUD overlay.

### Why Staggered Updates?

**Balance awareness vs cost** - injecting every turn would be expensive. Staggering (metrics every 5, trajectory every 3) provides regular updates without token waste.

---

## ðŸ”§ Advanced Usage

### Custom Phase Detection

Add your own workflow phases:

```python
# In modules/trajectory_analyzer.py

PHASES["code_review"] = {
    "description": "Reviewing code quality",
    "indicators": ["read_file", "grep", "python_check"],
    "weight": {"read_file": 2, "grep": 1, "python_check": 3},
}

TRAJECTORIES["implementation"] = ["verification", "code_review"]
```

### Export Metrics to External Systems

Extend `ObservabilityHook` for custom integrations:

```python
class PrometheusObservability(ObservabilityHook):
    async def on_provider_post(self, event: str, data: dict) -> HookResult:
        result = await super().on_provider_post(event, data)
        
        # Export to Prometheus
        prometheus_client.gauge('amplifier_cost').set(self.total_cost)
        prometheus_client.gauge('amplifier_tokens').set(sum(self.token_usage.values()))
        
        return result
```

---

## ðŸ’¡ Use Cases

### For Individual Developers
- **Budget management**: Track spending on API calls
- **Performance optimization**: Identify slow operations
- **Workflow understanding**: See how agents approach tasks
- **Debugging**: Detect when agents get stuck in loops

### For Teams
- **Cost allocation**: Track spending per project or user
- **Benchmark workflows**: Compare agent efficiency across tasks
- **Quality assurance**: Verify agents follow expected workflows
- **Training**: Understand agent behavior patterns

### For Research
- **Agent behavior analysis**: Study decision-making patterns
- **Efficiency metrics**: Compare different prompting strategies
- **Cost modeling**: Predict session costs from early signals

---

## ðŸ¤ Contributing

This bundle is part of the [Amplifier ecosystem](https://github.com/microsoft/amplifier). Contributions welcome!

**Development Setup:**

```bash
# Clone repository
git clone https://github.com/microsoft/amplifier-optimizer.git
cd amplifier-optimizer

# Run tests
python modules/observability/tests/test_basic_import.py

# Test with Amplifier
amplifier --bundle modules/observability
```

---

## ðŸ“š Documentation

- **[QUICKSTART.md](modules/observability/QUICKSTART.md)** - 5-minute setup guide
- **[Full README](modules/observability/README.md)** - Complete documentation
- **[Context Instructions](modules/observability/context/observability-instructions.md)** - How agents use this

---

## ðŸŒŸ Example Session Output

**Task:** "Add user authentication to my web app"

```
ðŸš€ Phase: Exploration â†’ Discovering codebase structure

[Agent searches files...]

ðŸš€ Trajectory: **exploration** â†’ analysis â†’ planning
ðŸ“Š Cost: $0.02 | Tokens: 4,567 | Time: 34s

ðŸš€ Phase: Analysis â†’ Understanding code and requirements

[Agent reads auth-related code...]

ðŸš€ Phase: Planning â†’ Designing solution architecture

[Agent designs OAuth2 flow...]

ðŸ’° Cost threshold exceeded: $0.10

ðŸš€ Phase: Implementation â†’ Writing code

[Agent writes auth code...]

ðŸš€ Phase: Verification â†’ Testing and validation

âš ï¸ Slow tool: bash took 11.3s

[Session ends]

================================================================================
ðŸ“Š SESSION SUMMARY
================================================================================

ðŸ’° Cost: $0.12
ðŸŽ¯ Tokens: 28,456
   â”œâ”€ Input:  18,234
   â””â”€ Output: 10,222

â±ï¸  Time: 512s (8.5 min)

ðŸ”§ Tool Usage:
   read_file            18Ã— calls, avg: 0.52s, total: 9.4s
   write_file            4Ã— calls, avg: 0.31s, total: 1.2s
   edit_file             2Ã— calls, avg: 0.28s, total: 0.6s
   bash                  3Ã— calls, avg: 5.82s, total: 17.5s
   LSP                   6Ã— calls, avg: 0.89s, total: 5.3s

âœ… OAuth2 authentication implemented and tested
```

---

## ðŸ“ˆ Metrics Tracked

### Cost Metrics
- Total API cost in USD
- Input/output token breakdown
- Per-turn cost
- Model-specific pricing

### Speed Metrics
- Tool execution timing
- Average/total time per tool type
- Session elapsed time
- Slow operation detection

### Trajectory Metrics
- Current workflow phase
- Phase confidence score
- Phase duration
- Predicted next phases
- Recent tool usage

---

## ðŸ§  How Phase Detection Works

The trajectory analyzer uses **weighted scoring** based on tool patterns:

| Phase | Key Indicators | Example Pattern |
|-------|----------------|-----------------|
| Exploration | glob, grep, read_file | `glob("**/*.py")` â†’ `read_file()` |
| Analysis | read_file, LSP, web_search | `LSP.hover()` â†’ `read_file()` |
| Planning | Few tools, long reasoning | Minimal tool use, long LLM responses |
| Implementation | write_file, edit_file | `write_file()` â†’ `edit_file()` |
| Verification | bash, python_check | `bash("pytest")` â†’ `python_check()` |
| Debugging | grep, read_file + errors | Tool errors â†’ intensive searching |

**Confidence threshold**: 60% required for phase transition to avoid false positives.

---

## ðŸŽ“ Design Decisions

### Why Hooks Instead of Tools?

**Hooks are passive** - they observe without requiring agent action. The agent gets automatic awareness without needing to remember to check metrics.

### Why Ephemeral Injection?

**Zero clutter** - metrics don't pollute conversation history. The agent has temporary awareness that updates automatically, like a HUD overlay in a video game.

### Why Staggered Updates?

**Balance awareness vs cost** - Metrics every 5 turns, trajectory every 3 turns. This provides regular updates without excessive token consumption.

### Why Weighted Phase Scoring?

**Robust detection** - Simple keyword matching is too brittle. Weighted scoring handles mixed tool usage and provides confidence scores.

---

## ðŸš¦ Requirements

- **Python**: 3.11+
- **Amplifier Core**: Latest version with hooks API
- **Dependencies**: None (uses stdlib only)

---

## ðŸ“„ License

MIT License - see [LICENSE](LICENSE) file for details

---

## ðŸ™ Acknowledgments

Built with â¤ï¸ following the Amplifier philosophy:

> *"The center stays still so the edges can move fast."*

Inspired by the need for transparency, awareness, and control in AI agent sessions.

---

## ðŸ”— Links

- **[Amplifier](https://github.com/microsoft/amplifier)** - The AI agent framework
- **[Amplifier Foundation](https://github.com/microsoft/amplifier-foundation)** - Bundle primitives and patterns
- **[Documentation](modules/observability/README.md)** - Full technical documentation

---

<div align="center">

**Give your AI agents mission control visibility** ðŸš€

[Get Started](#-quick-start) â€¢ [Documentation](modules/observability/README.md) â€¢ [Report Issue](https://github.com/microsoft/amplifier-optimizer/issues)

</div>
